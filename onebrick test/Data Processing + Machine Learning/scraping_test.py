# -*- coding: utf-8 -*-
"""scraping test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19Vvm5u2CdOQqrH2CNeBagsGF_fW9PmY1
"""

import requests

BASE_URL = 'https://fakestoreapi.com'

resp = requests.get(f"{BASE_URL}/products")
json_res =  resp.json()

number_of_elements = len(json_res)

i = 0
while i < number_of_elements:
   print(json_res[i]['title']) 
   print(json_res[i]['category'])
   print(json_res[i]['rating']['rate'])
   print("storeName")
   print(json_res[i]['price'])
   print(" ")
   i += 1

resp = requests.get(f"{BASE_URL}/products")
json_res =  resp.json()


df = pd.DataFrame(json_res)


df.groupby(['category']).groups.keys()

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
# %matplotlib inline
import plotly.express as px

#Libraries for preprocessing
from gensim.parsing.preprocessing import remove_stopwords
import string
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
import webcolors

#Download once if using NLTK for preprocessing
import nltk
nltk.download('punkt')

#Libraries for vectorisation
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.model_selection import GridSearchCV
from fuzzywuzzy import fuzz

#Libraries for clustering
from sklearn.cluster import KMeans

resp = requests.get(f"{BASE_URL}/products")
json_res =  resp.json()


df = pd.DataFrame(json_res)

text1 = df['title']

#Remove stopwords, punctuation and numbers
text2 = [remove_stopwords(x)\
        .translate(str.maketrans('','',string.punctuation))\
        .translate(str.maketrans('','',string.digits))\
        for x in text1]

text2

#make lower case
def stemSentence(sentence):
    porter = PorterStemmer()
    token_words = word_tokenize(sentence)
    stem_sentence = [porter.stem(word) for word in token_words]
    return ' '.join(stem_sentence)

text3 = pd.Series([stemSentence(x) for x in text2])

text3

#counts the occurrences of words in a document.
vectorizer_cv = CountVectorizer(analyzer='word')
X_cv = vectorizer_cv.fit_transform(text3)
X_cv

# Kmeans clutering
kmeans = KMeans(n_clusters=10)
kmeans.fit(X_cv)
result = pd.concat([text1,pd.DataFrame(X_cv.toarray(),columns=vectorizer_cv.get_feature_names())],axis=1)
result['cluster'] = kmeans.predict(X_cv)

result[['title','cluster']]